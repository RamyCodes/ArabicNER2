{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BtmgHNnvrbNI",
        "outputId": "fad74366-5e2c-4437-d7c9-ebf0a440862e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Oct 30 14:38:13 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Verify that you have the GPU recognized\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip uninstall torch torchtext torchvision torchvision torchdata torchaudio\n",
        "!pip install torch==1.13.0\n",
        "!pip install transformers==4.24.0\n",
        "!pip install torchtext==0.14.0\n",
        "!pip install torchvision==0.14.0\n",
        "!pip install torchdata==0.5.1\n",
        "!pip install seqeval==1.2.2"
      ],
      "metadata": {
        "id": "dtO91IXuujsZ",
        "outputId": "44db6110-6a05-4985-80d6-b779c1f17b9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 1.13.1\n",
            "Uninstalling torch-1.13.1:\n",
            "  Would remove:\n",
            "    /usr/local/bin/convert-caffe2-to-onnx\n",
            "    /usr/local/bin/convert-onnx-to-caffe2\n",
            "    /usr/local/bin/torchrun\n",
            "    /usr/local/lib/python3.10/dist-packages/functorch/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torch-1.13.1.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torch/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torchgen/*\n",
            "Proceed (Y/n)? n\n",
            "Found existing installation: torchtext 0.14.0\n",
            "Uninstalling torchtext-0.14.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/torchtext-0.14.0.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torchtext/*\n",
            "Proceed (Y/n)? n\n",
            "Found existing installation: torchvision 0.14.0\n",
            "Uninstalling torchvision-0.14.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/torchvision-0.14.0.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torchvision.libs/libcudart.782fcab0.so.11.0\n",
            "    /usr/local/lib/python3.10/dist-packages/torchvision.libs/libjpeg.ceea7512.so.62\n",
            "    /usr/local/lib/python3.10/dist-packages/torchvision.libs/libnvjpeg.9b083ad7.so.11\n",
            "    /usr/local/lib/python3.10/dist-packages/torchvision.libs/libpng16.7f72a3c5.so.16\n",
            "    /usr/local/lib/python3.10/dist-packages/torchvision.libs/libz.1328edc3.so.1\n",
            "    /usr/local/lib/python3.10/dist-packages/torchvision/*\n",
            "Proceed (Y/n)? n\n",
            "Found existing installation: torchdata 0.5.1\n",
            "Uninstalling torchdata-0.5.1:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/torchdata-0.5.1.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torchdata/*\n",
            "Proceed (Y/n)? n\n",
            "Found existing installation: torchaudio 2.1.0+cu118\n",
            "Uninstalling torchaudio-2.1.0+cu118:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/torchaudio-2.1.0+cu118.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torchaudio/*\n",
            "Proceed (Y/n)? n\n",
            "Collecting torch==1.13.0\n",
            "  Using cached torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl (890.1 MB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0) (4.5.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0) (0.41.2)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1\n",
            "    Uninstalling torch-1.13.1:\n",
            "      Successfully uninstalled torch-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 1.13.0 which is incompatible.\n",
            "torchdata 0.5.1 requires torch==1.13.1, but you have torch 1.13.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.13.0\n",
            "Requirement already satisfied: transformers==4.24.0 in /usr/local/lib/python3.10/dist-packages (4.24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (0.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.24.0) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.24.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.24.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.24.0) (2023.7.22)\n",
            "Requirement already satisfied: torchtext==0.14.0 in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.14.0) (4.66.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.14.0) (2.31.0)\n",
            "Requirement already satisfied: torch==1.13.0 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.14.0) (1.13.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.14.0) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext==0.14.0) (4.5.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext==0.14.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext==0.14.0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext==0.14.0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext==0.14.0) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchtext==0.14.0) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchtext==0.14.0) (0.41.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.14.0) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.14.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.14.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.14.0) (2023.7.22)\n",
            "Requirement already satisfied: torchvision==0.14.0 in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.0) (4.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.0) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.0) (2.31.0)\n",
            "Requirement already satisfied: torch==1.13.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.0) (1.13.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.0) (9.4.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchvision==0.14.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchvision==0.14.0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchvision==0.14.0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchvision==0.14.0) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchvision==0.14.0) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchvision==0.14.0) (0.41.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.0) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.0) (2023.7.22)\n",
            "Requirement already satisfied: torchdata==0.5.1 in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (2.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (2.31.0)\n",
            "Requirement already satisfied: portalocker>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (2.8.2)\n",
            "Collecting torch==1.13.1 (from torchdata==0.5.1)\n",
            "  Using cached torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->torchdata==0.5.1) (4.5.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->torchdata==0.5.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->torchdata==0.5.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->torchdata==0.5.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->torchdata==0.5.1) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchdata==0.5.1) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchdata==0.5.1) (0.41.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.5.1) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.5.1) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.5.1) (2023.7.22)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.0\n",
            "    Uninstalling torch-1.13.0:\n",
            "      Successfully uninstalled torch-1.13.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.14.0 requires torch==1.13.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchvision 0.14.0 requires torch==1.13.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.13.1\n",
            "Requirement already satisfied: seqeval==1.2.2 in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval==1.2.2) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval==1.2.2) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove existing package and clone again from Github\n",
        "!rm -rf /content/ArabicNER\n",
        "!git clone https://github.com/RamyCodes/ArabicNER.git"
      ],
      "metadata": {
        "id": "jLHHpnFXr7-K",
        "outputId": "1fd25f91-0400-419a-cbe9-262120cb365d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ArabicNER'...\n",
            "remote: Enumerating objects: 585, done.\u001b[K\n",
            "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 585 (delta 26), reused 30 (delta 14), pack-reused 526\u001b[K\n",
            "Receiving objects: 100% (585/585), 367.17 KiB | 3.71 MiB/s, done.\n",
            "Resolving deltas: 100% (339/339), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the ArabicNER package to the system path\n",
        "import sys\n",
        "import argparse\n",
        "sys.path.append('/content/ArabicNER/')"
      ],
      "metadata": {
        "id": "o4EvIlcrssU6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import train function\n",
        "from arabiner.bin.train import main as train"
      ],
      "metadata": {
        "id": "-rM-8p4nsztp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the model arguments\n",
        "args_dict = {\n",
        "    # Model output path to save artifacts and model predictions\n",
        "    \"output_path\": \"/content/output/\",\n",
        "\n",
        "    # train/test/validation data paths\n",
        "    \"train_path\": \"/content/ArabicNER/data/train.txt\",\n",
        "    \"test_path\": \"/content/ArabicNER/data/test.txt\",\n",
        "    \"val_path\": \"/content/ArabicNER/data/val.txt\",\n",
        "\n",
        "    # seed for randomization\n",
        "    \"seed\": 1,\n",
        "\n",
        "    \"batch_size\": 8,\n",
        "\n",
        "    # Nmber of workers for the dataloader\n",
        "    \"num_workers\": 1,\n",
        "\n",
        "    # GPU/device Ids to train model on\n",
        "    # For two GPUs use [0, 1]\n",
        "    # For three GPUs use [0, 1, 2], etc.\n",
        "    \"gpus\": [0],\n",
        "\n",
        "    # Overwrite data in output_path directory specified above\n",
        "    \"overwrite\": True,\n",
        "\n",
        "    # How often to print the logs in terms of number of steps\n",
        "    \"log_interval\": 10,\n",
        "\n",
        "    # Data configuration\n",
        "    # Here we specify the dataset class and there are two options:\n",
        "    #  arabiner.data.datasets.DefaultDataset: for flat NER\n",
        "    #  arabiner.data.datasets.NestedTagsDataset: for nested NER\n",
        "    #\n",
        "    # kwargs: keyword arguments to the dataset class\n",
        "    # This notebook used the DefaultDataset for flat NER\n",
        "    \"data_config\": {\n",
        "        \"fn\": \"arabiner.data.datasets.DefaultDataset\",\n",
        "        \"kwargs\": {\"max_seq_len\": 512}\n",
        "    },\n",
        "\n",
        "    # Neural net configuration\n",
        "    # There are two NNs:\n",
        "    #   arabiner.nn.BertSeqTagger: flat NER tagger\n",
        "    #   arabiner.nn.BertNestedTagger: nested NER tagger\n",
        "    #\n",
        "    # kwargs: keyword arguments to the NN\n",
        "    # This notebook uses BertSeqTagger for flat NER tagging\n",
        "    \"network_config\": {\n",
        "        \"fn\": \"arabiner.nn.BertSeqTagger\",\n",
        "        \"kwargs\": {\"dropout\": 0.1, \"bert_model\": \"aubmindlab/bert-base-arabertv2\"}\n",
        "    },\n",
        "\n",
        "    # Model trainer configuration\n",
        "    #\n",
        "    #  arabiner.trainers.BertTrainer: for flat NER training\n",
        "    #  arabiner.trainers.BertNestedTrainer: for nested NER training\n",
        "    #\n",
        "    # kwargs: keyword arguments to arabiner.trainers.BertTrainer\n",
        "    #         additional arguments you can pass includes\n",
        "    #           - clip: for gradient clpping\n",
        "    #           - patience: number of epochs for early termination\n",
        "    # This notebook uses BertTrainer for fat NER training\n",
        "    \"trainer_config\": {\n",
        "        \"fn\": \"arabiner.trainers.BertTrainer\",\n",
        "        \"kwargs\": {\"max_epochs\": 50}\n",
        "    },\n",
        "\n",
        "    # Optimizer configuration\n",
        "    # Our experiments use torch.optim.AdamW, however, you are free to pass\n",
        "    # any other optmizers such as torch.optim.Adam or torch.optim.SGD\n",
        "    # lr: learning rate\n",
        "    # kwargs: keyword arguments to torch.optim.AdamW or whatever optimizer you use\n",
        "    #\n",
        "    # Additional optimizers can be found here:\n",
        "    # https://pytorch.org/docs/stable/optim.html\n",
        "    \"optimizer\": {\n",
        "        \"fn\": \"torch.optim.AdamW\",\n",
        "        \"kwargs\": {\"lr\": 0.0001}\n",
        "    },\n",
        "\n",
        "    # Learning rate scheduler configuration\n",
        "    # You can pass a learning scheduler such as torch.optim.lr_scheduler.StepLR\n",
        "    # kwargs: keyword arguments to torch.optim.AdamW or whatever scheduler you use\n",
        "    #\n",
        "    # Additional schedulers can be found here:\n",
        "    # https://pytorch.org/docs/stable/optim.html\n",
        "    \"lr_scheduler\": {\n",
        "        \"fn\": \"torch.optim.lr_scheduler.ExponentialLR\",\n",
        "        \"kwargs\": {\"gamma\": 1}\n",
        "    },\n",
        "\n",
        "    # Loss function configuration\n",
        "    # We use cross entropy loss\n",
        "    # kwargs: keyword arguments to torch.nn.CrossEntropyLoss or whatever loss function you use\n",
        "    \"loss\": {\n",
        "        \"fn\": \"torch.nn.CrossEntropyLoss\",\n",
        "        \"kwargs\": {}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Convert args dictionary to argparse namespace\n",
        "args = argparse.Namespace()\n",
        "args.__dict__ = args_dict"
      ],
      "metadata": {
        "id": "UqqvY1fXtZpD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training the model\n",
        "train(args)"
      ],
      "metadata": {
        "id": "EJQMuAC8taZB",
        "outputId": "a8d37c26-9839-4d70-c2cc-bbe35a2a4769",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging to /content/output/train.log\n",
            "INFO\tarabiner.bin.train\tMon, 30 Oct 2023 14:40:27\tWriting config to /content/output/args.json\n",
            "INFO\tarabiner.utils.data\tMon, 30 Oct 2023 14:40:28\t78 batches found\n",
            "INFO\tarabiner.utils.data\tMon, 30 Oct 2023 14:40:28\t12 batches found\n",
            "INFO\tarabiner.utils.data\tMon, 30 Oct 2023 14:40:28\t23 batches found\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv2 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:40:44\tEpoch 0 | Batch 10/78 | Timestep 10 | LR 0.0001000000 | Loss 0.277689\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:40:46\tEpoch 0 | Batch 20/78 | Timestep 20 | LR 0.0001000000 | Loss 0.234022\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:40:48\tEpoch 0 | Batch 30/78 | Timestep 30 | LR 0.0001000000 | Loss 0.217355\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:40:50\tEpoch 0 | Batch 40/78 | Timestep 40 | LR 0.0001000000 | Loss 0.246373\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:40:52\tEpoch 0 | Batch 50/78 | Timestep 50 | LR 0.0001000000 | Loss 0.199122\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:40:54\tEpoch 0 | Batch 60/78 | Timestep 60 | LR 0.0001000000 | Loss 0.142319\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:40:56\tEpoch 0 | Batch 70/78 | Timestep 70 | LR 0.0001000000 | Loss 0.167207\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:40:58\t** Evaluating on validation dataset **\n",
            "INFO\troot\tMon, 30 Oct 2023 14:40:59\t\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC     0.0000    0.0000    0.0000        82\n",
            "        MISC     0.0000    0.0000    0.0000         6\n",
            "         ORG     0.0000    0.0000    0.0000        44\n",
            "        PERS     0.0000    0.0000    0.0000       137\n",
            "\n",
            "   micro avg     0.0000    0.0000    0.0000       269\n",
            "   macro avg     0.0000    0.0000    0.0000       269\n",
            "weighted avg     0.0000    0.0000    0.0000       269\n",
            "\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:40:59\tEpoch 0 | Timestep 78 | Train Loss 0.254514 | Val Loss 0.251774 | F1 0.000000\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:40:59\t** Validation improved, evaluating test data **\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO\troot\tMon, 30 Oct 2023 14:41:00\tPredictions written to /content/output/predictions.txt\n",
            "INFO\troot\tMon, 30 Oct 2023 14:41:00\t\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC     0.0000    0.0000    0.0000       175\n",
            "        MISC     0.0000    0.0000    0.0000        86\n",
            "         ORG     0.0000    0.0000    0.0000       109\n",
            "        PERS     0.0000    0.0000    0.0000       148\n",
            "\n",
            "   micro avg     0.0000    0.0000    0.0000       518\n",
            "   macro avg     0.0000    0.0000    0.0000       518\n",
            "weighted avg     0.0000    0.0000    0.0000       518\n",
            "\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:41:00\tEpoch 0 | Timestep 78 | Test Loss 0.301560 | F1 0.000000\n",
            "INFO\tarabiner.trainers.BaseTrainer\tMon, 30 Oct 2023 14:41:00\tSaving checkpoint to /content/output/checkpoints/checkpoint_0.pt\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:41:12\tEpoch 1 | Batch 2/78 | Timestep 80 | LR 0.0001000000 | Loss 0.165481\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:41:14\tEpoch 1 | Batch 12/78 | Timestep 90 | LR 0.0001000000 | Loss 0.115577\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:41:16\tEpoch 1 | Batch 22/78 | Timestep 100 | LR 0.0001000000 | Loss 0.229382\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:41:18\tEpoch 1 | Batch 32/78 | Timestep 110 | LR 0.0001000000 | Loss 0.360387\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:41:20\tEpoch 1 | Batch 42/78 | Timestep 120 | LR 0.0001000000 | Loss 0.229486\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:41:22\tEpoch 1 | Batch 52/78 | Timestep 130 | LR 0.0001000000 | Loss 0.149269\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:41:24\tEpoch 1 | Batch 62/78 | Timestep 140 | LR 0.0001000000 | Loss 0.169813\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:41:26\tEpoch 1 | Batch 72/78 | Timestep 150 | LR 0.0001000000 | Loss 0.202555\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:41:27\t** Evaluating on validation dataset **\n",
            "INFO\troot\tMon, 30 Oct 2023 14:41:28\t\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC     0.6552    0.4634    0.5429        82\n",
            "        MISC     0.0000    0.0000    0.0000         6\n",
            "         ORG     0.0000    0.0000    0.0000        44\n",
            "        PERS     0.7568    0.6131    0.6774       137\n",
            "\n",
            "   micro avg     0.7219    0.4535    0.5571       269\n",
            "   macro avg     0.3530    0.2691    0.3051       269\n",
            "weighted avg     0.5851    0.4535    0.5105       269\n",
            "\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:41:28\tEpoch 1 | Timestep 156 | Train Loss 0.169674 | Val Loss 0.140752 | F1 0.557078\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:41:28\t** Validation improved, evaluating test data **\n",
            "INFO\troot\tMon, 30 Oct 2023 14:41:29\tPredictions written to /content/output/predictions.txt\n",
            "INFO\troot\tMon, 30 Oct 2023 14:41:30\t\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC     0.5696    0.5143    0.5405       175\n",
            "        MISC     0.0000    0.0000    0.0000        86\n",
            "         ORG     0.0000    0.0000    0.0000       109\n",
            "        PERS     0.7009    0.5541    0.6189       148\n",
            "\n",
            "   micro avg     0.6255    0.3320    0.4338       518\n",
            "   macro avg     0.3176    0.2671    0.2899       518\n",
            "weighted avg     0.3927    0.3320    0.3594       518\n",
            "\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:41:30\tEpoch 1 | Timestep 156 | Test Loss 0.201347 | F1 0.433796\n",
            "INFO\tarabiner.trainers.BaseTrainer\tMon, 30 Oct 2023 14:41:30\tSaving checkpoint to /content/output/checkpoints/checkpoint_1.pt\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:41:37\tEpoch 2 | Batch 4/78 | Timestep 160 | LR 0.0001000000 | Loss 0.115267\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:41:39\tEpoch 2 | Batch 14/78 | Timestep 170 | LR 0.0001000000 | Loss 0.034714\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:41:41\tEpoch 2 | Batch 24/78 | Timestep 180 | LR 0.0001000000 | Loss 0.081706\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:41:43\tEpoch 2 | Batch 34/78 | Timestep 190 | LR 0.0001000000 | Loss 0.062848\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:41:45\tEpoch 2 | Batch 44/78 | Timestep 200 | LR 0.0001000000 | Loss 0.042711\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:41:47\tEpoch 2 | Batch 54/78 | Timestep 210 | LR 0.0001000000 | Loss 0.061917\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:41:49\tEpoch 2 | Batch 64/78 | Timestep 220 | LR 0.0001000000 | Loss 0.164106\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:41:51\tEpoch 2 | Batch 74/78 | Timestep 230 | LR 0.0001000000 | Loss 0.077549\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:41:52\t** Evaluating on validation dataset **\n",
            "INFO\troot\tMon, 30 Oct 2023 14:41:53\t\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC     0.3737    0.8659    0.5221        82\n",
            "        MISC     0.5000    0.1667    0.2500         6\n",
            "         ORG     0.2222    0.1364    0.1690        44\n",
            "        PERS     0.7563    0.6569    0.7031       137\n",
            "\n",
            "   micro avg     0.4970    0.6245    0.5535       269\n",
            "   macro avg     0.4631    0.4565    0.4110       269\n",
            "weighted avg     0.5466    0.6245    0.5505       269\n",
            "\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:41:53\tEpoch 2 | Timestep 234 | Train Loss 0.083115 | Val Loss 0.087720 | F1 0.553542\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:41:53\t** Validation improved, evaluating test data **\n",
            "INFO\troot\tMon, 30 Oct 2023 14:41:54\tPredictions written to /content/output/predictions.txt\n",
            "INFO\troot\tMon, 30 Oct 2023 14:41:54\t\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC     0.3468    0.8343    0.4899       175\n",
            "        MISC     0.7500    0.0349    0.0667        86\n",
            "         ORG     0.1364    0.0275    0.0458       109\n",
            "        PERS     0.6644    0.6554    0.6599       148\n",
            "\n",
            "   micro avg     0.4199    0.4807    0.4482       518\n",
            "   macro avg     0.4744    0.3880    0.3156       518\n",
            "weighted avg     0.4602    0.4807    0.3748       518\n",
            "\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:41:54\tEpoch 2 | Timestep 234 | Test Loss 0.163256 | F1 0.448245\n",
            "INFO\tarabiner.trainers.BaseTrainer\tMon, 30 Oct 2023 14:41:54\tSaving checkpoint to /content/output/checkpoints/checkpoint_2.pt\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:42:06\tEpoch 3 | Batch 6/78 | Timestep 240 | LR 0.0001000000 | Loss 0.061278\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:42:08\tEpoch 3 | Batch 16/78 | Timestep 250 | LR 0.0001000000 | Loss 0.064742\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:42:10\tEpoch 3 | Batch 26/78 | Timestep 260 | LR 0.0001000000 | Loss 0.065536\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:42:12\tEpoch 3 | Batch 36/78 | Timestep 270 | LR 0.0001000000 | Loss 0.041667\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:42:14\tEpoch 3 | Batch 46/78 | Timestep 280 | LR 0.0001000000 | Loss 0.037462\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:42:16\tEpoch 3 | Batch 56/78 | Timestep 290 | LR 0.0001000000 | Loss 0.046308\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:42:18\tEpoch 3 | Batch 66/78 | Timestep 300 | LR 0.0001000000 | Loss 0.039823\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:42:20\tEpoch 3 | Batch 76/78 | Timestep 310 | LR 0.0001000000 | Loss 0.056119\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:42:21\t** Evaluating on validation dataset **\n",
            "INFO\troot\tMon, 30 Oct 2023 14:42:22\t\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC     0.6087    0.8537    0.7107        82\n",
            "        MISC     0.5000    0.1667    0.2500         6\n",
            "         ORG     0.5625    0.4091    0.4737        44\n",
            "        PERS     0.8411    0.6569    0.7377       137\n",
            "\n",
            "   micro avg     0.6992    0.6654    0.6819       269\n",
            "   macro avg     0.6281    0.5216    0.5430       269\n",
            "weighted avg     0.7171    0.6654    0.6754       269\n",
            "\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:42:22\tEpoch 3 | Timestep 312 | Train Loss 0.049112 | Val Loss 0.071834 | F1 0.681905\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:42:22\t** Validation improved, evaluating test data **\n",
            "INFO\troot\tMon, 30 Oct 2023 14:42:23\tPredictions written to /content/output/predictions.txt\n",
            "INFO\troot\tMon, 30 Oct 2023 14:42:23\t\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC     0.6250    0.8857    0.7329       175\n",
            "        MISC     0.5556    0.1163    0.1923        86\n",
            "         ORG     0.4638    0.2936    0.3596       109\n",
            "        PERS     0.7965    0.6081    0.6897       148\n",
            "\n",
            "   micro avg     0.6406    0.5541    0.5942       518\n",
            "   macro avg     0.6102    0.4759    0.4936       518\n",
            "weighted avg     0.6285    0.5541    0.5522       518\n",
            "\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:42:23\tEpoch 3 | Timestep 312 | Test Loss 0.131773 | F1 0.594203\n",
            "INFO\tarabiner.trainers.BaseTrainer\tMon, 30 Oct 2023 14:42:23\tSaving checkpoint to /content/output/checkpoints/checkpoint_3.pt\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:42:35\tEpoch 4 | Batch 8/78 | Timestep 320 | LR 0.0001000000 | Loss 0.018603\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:42:37\tEpoch 4 | Batch 18/78 | Timestep 330 | LR 0.0001000000 | Loss 0.042489\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:42:39\tEpoch 4 | Batch 28/78 | Timestep 340 | LR 0.0001000000 | Loss 0.024290\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:42:41\tEpoch 4 | Batch 38/78 | Timestep 350 | LR 0.0001000000 | Loss 0.027565\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:42:43\tEpoch 4 | Batch 48/78 | Timestep 360 | LR 0.0001000000 | Loss 0.055713\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:42:45\tEpoch 4 | Batch 58/78 | Timestep 370 | LR 0.0001000000 | Loss 0.009034\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:42:47\tEpoch 4 | Batch 68/78 | Timestep 380 | LR 0.0001000000 | Loss 0.013429\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:42:49\tEpoch 4 | Batch 78/78 | Timestep 390 | LR 0.0001000000 | Loss 0.080614\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:42:49\t** Evaluating on validation dataset **\n",
            "INFO\troot\tMon, 30 Oct 2023 14:42:50\t\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC     0.7396    0.8659    0.7978        82\n",
            "        MISC     0.2857    0.3333    0.3077         6\n",
            "         ORG     0.5814    0.5682    0.5747        44\n",
            "        PERS     0.8279    0.7372    0.7799       137\n",
            "\n",
            "   micro avg     0.7425    0.7398    0.7412       269\n",
            "   macro avg     0.6086    0.6261    0.6150       269\n",
            "weighted avg     0.7485    0.7398    0.7413       269\n",
            "\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:42:50\tEpoch 4 | Timestep 390 | Train Loss 0.026565 | Val Loss 0.051412 | F1 0.741155\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:42:50\t** Validation improved, evaluating test data **\n",
            "INFO\troot\tMon, 30 Oct 2023 14:42:51\tPredictions written to /content/output/predictions.txt\n",
            "INFO\troot\tMon, 30 Oct 2023 14:42:52\t\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC     0.7246    0.8571    0.7853       175\n",
            "        MISC     0.7188    0.2674    0.3898        86\n",
            "         ORG     0.5000    0.3945    0.4410       109\n",
            "        PERS     0.8182    0.6689    0.7361       148\n",
            "\n",
            "   micro avg     0.7063    0.6081    0.6535       518\n",
            "   macro avg     0.6904    0.5470    0.5881       518\n",
            "weighted avg     0.7031    0.6081    0.6331       518\n",
            "\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:42:52\tEpoch 4 | Timestep 390 | Test Loss 0.123312 | F1 0.653527\n",
            "INFO\tarabiner.trainers.BaseTrainer\tMon, 30 Oct 2023 14:42:52\tSaving checkpoint to /content/output/checkpoints/checkpoint_4.pt\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:00\tEpoch 5 | Batch 10/78 | Timestep 400 | LR 0.0001000000 | Loss 0.015783\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:02\tEpoch 5 | Batch 20/78 | Timestep 410 | LR 0.0001000000 | Loss 0.032934\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:04\tEpoch 5 | Batch 30/78 | Timestep 420 | LR 0.0001000000 | Loss 0.004158\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:06\tEpoch 5 | Batch 40/78 | Timestep 430 | LR 0.0001000000 | Loss 0.015304\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:08\tEpoch 5 | Batch 50/78 | Timestep 440 | LR 0.0001000000 | Loss 0.031029\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:10\tEpoch 5 | Batch 60/78 | Timestep 450 | LR 0.0001000000 | Loss 0.016707\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:12\tEpoch 5 | Batch 70/78 | Timestep 460 | LR 0.0001000000 | Loss 0.020596\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:14\t** Evaluating on validation dataset **\n",
            "INFO\troot\tMon, 30 Oct 2023 14:43:15\t\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC     0.6952    0.8902    0.7807        82\n",
            "        MISC     0.2222    0.3333    0.2667         6\n",
            "         ORG     0.3625    0.6591    0.4677        44\n",
            "        PERS     0.8750    0.5620    0.6844       137\n",
            "\n",
            "   micro avg     0.6418    0.6729    0.6570       269\n",
            "   macro avg     0.5387    0.6112    0.5499       269\n",
            "weighted avg     0.7218    0.6729    0.6690       269\n",
            "\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:15\tEpoch 5 | Timestep 468 | Train Loss 0.015208 | Val Loss 0.087420 | F1 0.656987\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:15\tEpoch 6 | Batch 2/78 | Timestep 470 | LR 0.0001000000 | Loss 0.002798\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:17\tEpoch 6 | Batch 12/78 | Timestep 480 | LR 0.0001000000 | Loss 0.008933\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:20\tEpoch 6 | Batch 22/78 | Timestep 490 | LR 0.0001000000 | Loss 0.004577\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:22\tEpoch 6 | Batch 32/78 | Timestep 500 | LR 0.0001000000 | Loss 0.006873\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:23\tEpoch 6 | Batch 42/78 | Timestep 510 | LR 0.0001000000 | Loss 0.008017\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:25\tEpoch 6 | Batch 52/78 | Timestep 520 | LR 0.0001000000 | Loss 0.021455\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:28\tEpoch 6 | Batch 62/78 | Timestep 530 | LR 0.0001000000 | Loss 0.010960\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:29\tEpoch 6 | Batch 72/78 | Timestep 540 | LR 0.0001000000 | Loss 0.005538\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:31\t** Evaluating on validation dataset **\n",
            "INFO\troot\tMon, 30 Oct 2023 14:43:32\t\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC     0.7273    0.8780    0.7956        82\n",
            "        MISC     0.1667    0.1667    0.1667         6\n",
            "         ORG     0.4328    0.6591    0.5225        44\n",
            "        PERS     0.8818    0.7080    0.7854       137\n",
            "\n",
            "   micro avg     0.7057    0.7398    0.7223       269\n",
            "   macro avg     0.5521    0.6030    0.5675       269\n",
            "weighted avg     0.7453    0.7398    0.7317       269\n",
            "\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:32\tEpoch 6 | Timestep 546 | Train Loss 0.012351 | Val Loss 0.070230 | F1 0.722323\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:33\tEpoch 7 | Batch 4/78 | Timestep 550 | LR 0.0001000000 | Loss 0.003556\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:36\tEpoch 7 | Batch 14/78 | Timestep 560 | LR 0.0001000000 | Loss 0.002305\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:38\tEpoch 7 | Batch 24/78 | Timestep 570 | LR 0.0001000000 | Loss 0.023929\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:40\tEpoch 7 | Batch 34/78 | Timestep 580 | LR 0.0001000000 | Loss 0.019709\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:42\tEpoch 7 | Batch 44/78 | Timestep 590 | LR 0.0001000000 | Loss 0.014064\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:44\tEpoch 7 | Batch 54/78 | Timestep 600 | LR 0.0001000000 | Loss 0.024337\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:45\tEpoch 7 | Batch 64/78 | Timestep 610 | LR 0.0001000000 | Loss 0.013922\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:48\tEpoch 7 | Batch 74/78 | Timestep 620 | LR 0.0001000000 | Loss 0.012322\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:48\t** Evaluating on validation dataset **\n",
            "INFO\troot\tMon, 30 Oct 2023 14:43:50\t\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC     0.7701    0.8171    0.7929        82\n",
            "        MISC     0.2222    0.3333    0.2667         6\n",
            "         ORG     0.4884    0.4773    0.4828        44\n",
            "        PERS     0.8614    0.6350    0.7311       137\n",
            "\n",
            "   micro avg     0.7375    0.6580    0.6955       269\n",
            "   macro avg     0.5855    0.5657    0.5684       269\n",
            "weighted avg     0.7583    0.6580    0.6990       269\n",
            "\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:50\tEpoch 7 | Timestep 624 | Train Loss 0.012865 | Val Loss 0.086796 | F1 0.695481\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:51\tEpoch 8 | Batch 6/78 | Timestep 630 | LR 0.0001000000 | Loss 0.003239\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:53\tEpoch 8 | Batch 16/78 | Timestep 640 | LR 0.0001000000 | Loss 0.002429\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:55\tEpoch 8 | Batch 26/78 | Timestep 650 | LR 0.0001000000 | Loss 0.004923\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:57\tEpoch 8 | Batch 36/78 | Timestep 660 | LR 0.0001000000 | Loss 0.001862\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:43:59\tEpoch 8 | Batch 46/78 | Timestep 670 | LR 0.0001000000 | Loss 0.004666\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:44:01\tEpoch 8 | Batch 56/78 | Timestep 680 | LR 0.0001000000 | Loss 0.009050\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:44:03\tEpoch 8 | Batch 66/78 | Timestep 690 | LR 0.0001000000 | Loss 0.006670\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:44:06\tEpoch 8 | Batch 76/78 | Timestep 700 | LR 0.0001000000 | Loss 0.003239\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:44:06\t** Evaluating on validation dataset **\n",
            "INFO\troot\tMon, 30 Oct 2023 14:44:07\t\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC     0.8000    0.8293    0.8144        82\n",
            "        MISC     0.2000    0.1667    0.1818         6\n",
            "         ORG     0.6222    0.6364    0.6292        44\n",
            "        PERS     0.8319    0.7226    0.7734       137\n",
            "\n",
            "   micro avg     0.7717    0.7286    0.7495       269\n",
            "   macro avg     0.6135    0.5887    0.5997       269\n",
            "weighted avg     0.7738    0.7286    0.7491       269\n",
            "\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:44:07\tEpoch 8 | Timestep 702 | Train Loss 0.009585 | Val Loss 0.067944 | F1 0.749522\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:44:09\tEpoch 9 | Batch 8/78 | Timestep 710 | LR 0.0001000000 | Loss 0.011498\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:44:11\tEpoch 9 | Batch 18/78 | Timestep 720 | LR 0.0001000000 | Loss 0.014518\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:44:13\tEpoch 9 | Batch 28/78 | Timestep 730 | LR 0.0001000000 | Loss 0.002196\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:44:15\tEpoch 9 | Batch 38/78 | Timestep 740 | LR 0.0001000000 | Loss 0.011899\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:44:17\tEpoch 9 | Batch 48/78 | Timestep 750 | LR 0.0001000000 | Loss 0.003816\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:44:19\tEpoch 9 | Batch 58/78 | Timestep 760 | LR 0.0001000000 | Loss 0.002021\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:44:21\tEpoch 9 | Batch 68/78 | Timestep 770 | LR 0.0001000000 | Loss 0.008812\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:44:23\tEpoch 9 | Batch 78/78 | Timestep 780 | LR 0.0001000000 | Loss 0.002169\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:44:24\t** Evaluating on validation dataset **\n",
            "INFO\troot\tMon, 30 Oct 2023 14:44:24\t\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC     0.7614    0.8171    0.7882        82\n",
            "        MISC     0.1250    0.1667    0.1429         6\n",
            "         ORG     0.5682    0.5682    0.5682        44\n",
            "        PERS     0.8099    0.7153    0.7597       137\n",
            "\n",
            "   micro avg     0.7318    0.7100    0.7208       269\n",
            "   macro avg     0.5661    0.5668    0.5647       269\n",
            "weighted avg     0.7403    0.7100    0.7233       269\n",
            "\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:44:24\tEpoch 9 | Timestep 780 | Train Loss 0.006286 | Val Loss 0.075819 | F1 0.720755\n",
            "INFO\tarabiner.trainers.BertTrainer\tMon, 30 Oct 2023 14:44:24\tEarly termination triggered\n"
          ]
        }
      ]
    }
  ]
}
